package typings.tensorflowTfjsConverter

import typings.tensorflowTfjsConverter.anon.Typeofio
import typings.tensorflowTfjsConverter.distDataTypesMod.NamedTensorsMap
import typings.tensorflowTfjsConverter.distDataTypesMod.TensorInfo
import typings.tensorflowTfjsCore.distIoTypesMod.IOHandler
import typings.tensorflowTfjsCore.distIoTypesMod.IOHandlerSync
import typings.tensorflowTfjsCore.distIoTypesMod.LoadOptions
import typings.tensorflowTfjsCore.distIoTypesMod.ModelArtifacts
import typings.tensorflowTfjsCore.distIoTypesMod.ModelJSON
import typings.tensorflowTfjsCore.distIoTypesMod.SaveConfig
import typings.tensorflowTfjsCore.distIoTypesMod.SaveResult
import typings.tensorflowTfjsCore.distModelTypesMod.InferenceModel
import typings.tensorflowTfjsCore.distModelTypesMod.ModelPredictConfig
import typings.tensorflowTfjsCore.distTensorMod.Tensor
import typings.tensorflowTfjsCore.distTensorTypesMod.NamedTensorMap
import typings.tensorflowTfjsCore.distTypesMod.Rank
import org.scalablytyped.runtime.StObject
import scala.scalajs.js
import scala.scalajs.js.annotation.{JSGlobalScope, JSGlobal, JSImport, JSName, JSBracketAccess}

object distExecutorGraphModelMod {
  
  @JSImport("@tensorflow/tfjs-converter/dist/executor/graph_model", JSImport.Namespace)
  @js.native
  val ^ : js.Any = js.native
  
  @JSImport("@tensorflow/tfjs-converter/dist/executor/graph_model", "DEFAULT_MODEL_NAME")
  @js.native
  val DEFAULT_MODEL_NAME: /* "model.json" */ String = js.native
  
  @JSImport("@tensorflow/tfjs-converter/dist/executor/graph_model", "GraphModel")
  @js.native
  open class GraphModel[ModelURL /* <: Url */] protected ()
    extends StObject
       with InferenceModel {
    /**
      * @param modelUrl url for the model, or an `io.IOHandler`.
      * @param weightManifestUrl url for the weight file generated by
      * scripts/convert.py script.
      * @param requestOption options for Request, which allows to send credentials
      * and custom headers.
      * @param onProgress Optional, progress callback function, fired periodically
      * before the load is completed.
      */
    def this(modelUrl: ModelURL) = this()
    def this(modelUrl: ModelURL, loadOptions: LoadOptions) = this()
    def this(modelUrl: ModelURL, loadOptions: Unit, tfio: Typeofio) = this()
    def this(modelUrl: ModelURL, loadOptions: LoadOptions, tfio: Typeofio) = this()
    
    /* private */ var addStructuredOutputNames: Any = js.native
    
    /* private */ var artifacts: Any = js.native
    
    /* private */ var convertTensorMapToTensorsMap: Any = js.native
    
    /**
      * Releases the memory used by the weight tensors and resourceManager.
      *
      * @doc {heading: 'Models', subheading: 'Classes'}
      */
    def dispose(): Unit = js.native
    
    /**
      * Dispose intermediate tensors for model debugging mode (flag
      * KEEP_INTERMEDIATE_TENSORS is true).
      *
      * @doc {heading: 'Models', subheading: 'Classes'}
      */
    def disposeIntermediateTensors(): Unit = js.native
    
    def execute(inputs: js.Array[Tensor[Rank]]): Tensor[Rank] | js.Array[Tensor[Rank]] = js.native
    /**
      * Executes inference for the model for given input tensors.
      * @param inputs tensor, tensor array or tensor map of the inputs for the
      * model, keyed by the input node names.
      * @param outputs output node name from the TensorFlow model, if no
      * outputs are specified, the default outputs of the model would be used.
      * You can inspect intermediate nodes of the model by adding them to the
      * outputs array.
      *
      * @returns A single tensor if provided with a single output or no outputs
      * are provided and there is only one default output, otherwise return a
      * tensor array. The order of the tensor array is the same as the outputs
      * if provided, otherwise the order of outputNodes attribute of the model.
      *
      * @doc {heading: 'Models', subheading: 'Classes'}
      */
    def execute(inputs: Tensor[Rank]): Tensor[Rank] | js.Array[Tensor[Rank]] = js.native
    def execute(inputs: NamedTensorMap): Tensor[Rank] | js.Array[Tensor[Rank]] = js.native
    
    def executeAsync(inputs: js.Array[Tensor[Rank]]): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    def executeAsync(inputs: js.Array[Tensor[Rank]], outputs: String): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    def executeAsync(inputs: js.Array[Tensor[Rank]], outputs: js.Array[String]): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    /**
      * Executes inference for the model for given input tensors in async
      * fashion, use this method when your model contains control flow ops.
      * @param inputs tensor, tensor array or tensor map of the inputs for the
      * model, keyed by the input node names.
      * @param outputs output node name from the TensorFlow model, if no outputs
      * are specified, the default outputs of the model would be used. You can
      * inspect intermediate nodes of the model by adding them to the outputs
      * array.
      *
      * @returns A Promise of single tensor if provided with a single output or
      * no outputs are provided and there is only one default output, otherwise
      * return a tensor map.
      *
      * @doc {heading: 'Models', subheading: 'Classes'}
      */
    def executeAsync(inputs: Tensor[Rank]): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    def executeAsync(inputs: Tensor[Rank], outputs: String): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    def executeAsync(inputs: Tensor[Rank], outputs: js.Array[String]): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    def executeAsync(inputs: NamedTensorMap): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    def executeAsync(inputs: NamedTensorMap, outputs: String): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    def executeAsync(inputs: NamedTensorMap, outputs: js.Array[String]): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    
    /* private */ var executeInitializerGraph: Any = js.native
    
    /* private */ var executeInitializerGraphAsync: Any = js.native
    
    /* private */ var executor: Any = js.native
    
    /* private */ var findIOHandler: Any = js.native
    
    /**
      * Get intermediate tensors for model debugging mode (flag
      * KEEP_INTERMEDIATE_TENSORS is true).
      *
      * @doc {heading: 'Models', subheading: 'Classes'}
      */
    def getIntermediateTensors(): NamedTensorsMap = js.native
    
    /* private */ var handler: Any = js.native
    
    /* private */ var initializer: Any = js.native
    
    /* private */ var initializerSignature: Any = js.native
    
    def inputNodes: js.Array[String] = js.native
    
    @JSName("inputs")
    def inputs_MGraphModel: js.Array[TensorInfo] = js.native
    
    /* private */ val io: Any = js.native
    
    /**
      * Loads the model and weight files, construct the in memory weight map and
      * compile the inference graph.
      */
    def load(): /* import warning: importer.ImportType#apply Failed type conversion: @tensorflow/tfjs-converter.@tensorflow/tfjs-converter/dist/executor/graph_model.UrlIOHandler<ModelURL> extends @tensorflow/tfjs-core.@tensorflow/tfjs-core/dist/io/types.IOHandlerSync ? boolean : std.Promise<boolean> */ js.Any = js.native
    
    /* private */ var loadOptions: Any = js.native
    
    /**
      * Synchronously construct the in memory weight map and
      * compile the inference graph.
      *
      * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}
      */
    def loadSync(artifacts: ModelArtifacts): Boolean = js.native
    
    def metadata: js.Object = js.native
    
    def modelSignature: js.Object = js.native
    
    def modelStructuredOutputKeys: js.Object = js.native
    
    /* private */ var modelUrl: Any = js.native
    
    def modelVersion: String = js.native
    
    /* private */ var normalizeInputs: Any = js.native
    
    /* private */ var normalizeOutputs: Any = js.native
    
    def outputNodes: js.Array[String] = js.native
    
    @JSName("outputs")
    def outputs_MGraphModel: js.Array[TensorInfo] = js.native
    
    def predict(inputs: js.Array[Tensor[Rank]]): Tensor[Rank] | js.Array[Tensor[Rank]] | NamedTensorMap = js.native
    /**
      * Execute the inference for the input tensors.
      *
      * @param input The input tensors, when there is single input for the model,
      * inputs param should be a `tf.Tensor`. For models with mutliple inputs,
      * inputs params should be in either `tf.Tensor`[] if the input order is
      * fixed, or otherwise NamedTensorMap format.
      *
      * For model with multiple inputs, we recommend you use NamedTensorMap as the
      * input type, if you use `tf.Tensor`[], the order of the array needs to
      * follow the
      * order of inputNodes array. @see {@link GraphModel.inputNodes}
      *
      * You can also feed any intermediate nodes using the NamedTensorMap as the
      * input type. For example, given the graph
      *    InputNode => Intermediate => OutputNode,
      * you can execute the subgraph Intermediate => OutputNode by calling
      *    model.execute('IntermediateNode' : tf.tensor(...));
      *
      * This is useful for models that uses tf.dynamic_rnn, where the intermediate
      * state needs to be fed manually.
      *
      * For batch inference execution, the tensors for each input need to be
      * concatenated together. For example with mobilenet, the required input shape
      * is [1, 244, 244, 3], which represents the [batch, height, width, channel].
      * If we are provide a batched data of 100 images, the input tensor should be
      * in the shape of [100, 244, 244, 3].
      *
      * @param config Prediction configuration for specifying the batch size.
      * Currently the batch size option is ignored for graph model.
      *
      * @returns Inference result tensors. If the model is converted and it
      * originally had structured_outputs in tensorflow, then a NamedTensorMap
      * will be returned matching the structured_outputs. If no structured_outputs
      * are present, the output will be single `tf.Tensor` if the model has single
      * output node, otherwise Tensor[].
      *
      * @doc {heading: 'Models', subheading: 'Classes'}
      */
    def predict(inputs: Tensor[Rank]): Tensor[Rank] | js.Array[Tensor[Rank]] | NamedTensorMap = js.native
    def predict(inputs: NamedTensorMap): Tensor[Rank] | js.Array[Tensor[Rank]] | NamedTensorMap = js.native
    
    def predictAsync(inputs: js.Array[Tensor[Rank]]): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]] | NamedTensorMap] = js.native
    def predictAsync(inputs: js.Array[Tensor[Rank]], config: ModelPredictConfig): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]] | NamedTensorMap] = js.native
    /**
      * Execute the inference for the input tensors in async fashion, use this
      * method when your model contains control flow ops.
      *
      * @param input The input tensors, when there is single input for the model,
      * inputs param should be a `tf.Tensor`. For models with mutliple inputs,
      * inputs params should be in either `tf.Tensor`[] if the input order is
      * fixed, or otherwise NamedTensorMap format.
      *
      * For model with multiple inputs, we recommend you use NamedTensorMap as the
      * input type, if you use `tf.Tensor`[], the order of the array needs to
      * follow the
      * order of inputNodes array. @see {@link GraphModel.inputNodes}
      *
      * You can also feed any intermediate nodes using the NamedTensorMap as the
      * input type. For example, given the graph
      *    InputNode => Intermediate => OutputNode,
      * you can execute the subgraph Intermediate => OutputNode by calling
      *    model.execute('IntermediateNode' : tf.tensor(...));
      *
      * This is useful for models that uses tf.dynamic_rnn, where the intermediate
      * state needs to be fed manually.
      *
      * For batch inference execution, the tensors for each input need to be
      * concatenated together. For example with mobilenet, the required input shape
      * is [1, 244, 244, 3], which represents the [batch, height, width, channel].
      * If we are provide a batched data of 100 images, the input tensor should be
      * in the shape of [100, 244, 244, 3].
      *
      * @param config Prediction configuration for specifying the batch size.
      * Currently the batch size option is ignored for graph model.
      *
      * @returns A Promise of inference result tensors. If the model is converted
      * and it originally had structured_outputs in tensorflow, then a
      * NamedTensorMap will be returned matching the structured_outputs. If no
      * structured_outputs are present, the output will be single `tf.Tensor` if
      * the model has single output node, otherwise Tensor[].
      *
      * @doc {heading: 'Models', subheading: 'Classes'}
      */
    def predictAsync(inputs: Tensor[Rank]): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]] | NamedTensorMap] = js.native
    def predictAsync(inputs: Tensor[Rank], config: ModelPredictConfig): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]] | NamedTensorMap] = js.native
    def predictAsync(inputs: NamedTensorMap): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]] | NamedTensorMap] = js.native
    def predictAsync(inputs: NamedTensorMap, config: ModelPredictConfig): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]] | NamedTensorMap] = js.native
    
    /* private */ var resourceIdToCapturedInput: Any = js.native
    
    /* private */ var resourceManager: Any = js.native
    
    def save(handlerOrURL: String): js.Promise[SaveResult] = js.native
    def save(handlerOrURL: String, config: SaveConfig): js.Promise[SaveResult] = js.native
    /**
      * Save the configuration and/or weights of the GraphModel.
      *
      * An `IOHandler` is an object that has a `save` method of the proper
      * signature defined. The `save` method manages the storing or
      * transmission of serialized data ("artifacts") that represent the
      * model's topology and weights onto or via a specific medium, such as
      * file downloads, local storage, IndexedDB in the web browser and HTTP
      * requests to a server. TensorFlow.js provides `IOHandler`
      * implementations for a number of frequently used saving mediums, such as
      * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`
      * for more details.
      *
      * This method also allows you to refer to certain types of `IOHandler`s
      * as URL-like string shortcuts, such as 'localstorage://' and
      * 'indexeddb://'.
      *
      * Example 1: Save `model`'s topology and weights to browser [local
      * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);
      * then load it back.
      *
      * ```js
      * const modelUrl =
      *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';
      * const model = await tf.loadGraphModel(modelUrl);
      * const zeros = tf.zeros([1, 224, 224, 3]);
      * model.predict(zeros).print();
      *
      * const saveResults = await model.save('localstorage://my-model-1');
      *
      * const loadedModel = await tf.loadGraphModel('localstorage://my-model-1');
      * console.log('Prediction from loaded model:');
      * model.predict(zeros).print();
      * ```
      *
      * @param handlerOrURL An instance of `IOHandler` or a URL-like,
      * scheme-based string shortcut for `IOHandler`.
      * @param config Options for saving the model.
      * @returns A `Promise` of `SaveResult`, which summarizes the result of
      * the saving, such as byte sizes of the saved artifacts for the model's
      *   topology and weight values.
      *
      * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}
      */
    def save(handlerOrURL: IOHandler): js.Promise[SaveResult] = js.native
    def save(handlerOrURL: IOHandler, config: SaveConfig): js.Promise[SaveResult] = js.native
    
    /* private */ var setResourceIdToCapturedInput: Any = js.native
    
    /* private */ var signature: Any = js.native
    
    /* private */ var structuredOutputKeys: Any = js.native
    
    /* private */ var version: Any = js.native
    
    def weights: NamedTensorsMap = js.native
  }
  
  @JSImport("@tensorflow/tfjs-converter/dist/executor/graph_model", "TFHUB_SEARCH_PARAM")
  @js.native
  val TFHUB_SEARCH_PARAM: /* "?tfjs-format=file" */ String = js.native
  
  inline def loadGraphModel(modelUrl: String): js.Promise[GraphModel[String | IOHandler]] = ^.asInstanceOf[js.Dynamic].applyDynamic("loadGraphModel")(modelUrl.asInstanceOf[js.Any]).asInstanceOf[js.Promise[GraphModel[String | IOHandler]]]
  inline def loadGraphModel(modelUrl: String, options: Unit, tfio: Typeofio): js.Promise[GraphModel[String | IOHandler]] = (^.asInstanceOf[js.Dynamic].applyDynamic("loadGraphModel")(modelUrl.asInstanceOf[js.Any], options.asInstanceOf[js.Any], tfio.asInstanceOf[js.Any])).asInstanceOf[js.Promise[GraphModel[String | IOHandler]]]
  inline def loadGraphModel(modelUrl: String, options: LoadOptions): js.Promise[GraphModel[String | IOHandler]] = (^.asInstanceOf[js.Dynamic].applyDynamic("loadGraphModel")(modelUrl.asInstanceOf[js.Any], options.asInstanceOf[js.Any])).asInstanceOf[js.Promise[GraphModel[String | IOHandler]]]
  inline def loadGraphModel(modelUrl: String, options: LoadOptions, tfio: Typeofio): js.Promise[GraphModel[String | IOHandler]] = (^.asInstanceOf[js.Dynamic].applyDynamic("loadGraphModel")(modelUrl.asInstanceOf[js.Any], options.asInstanceOf[js.Any], tfio.asInstanceOf[js.Any])).asInstanceOf[js.Promise[GraphModel[String | IOHandler]]]
  inline def loadGraphModel(modelUrl: IOHandler): js.Promise[GraphModel[String | IOHandler]] = ^.asInstanceOf[js.Dynamic].applyDynamic("loadGraphModel")(modelUrl.asInstanceOf[js.Any]).asInstanceOf[js.Promise[GraphModel[String | IOHandler]]]
  inline def loadGraphModel(modelUrl: IOHandler, options: Unit, tfio: Typeofio): js.Promise[GraphModel[String | IOHandler]] = (^.asInstanceOf[js.Dynamic].applyDynamic("loadGraphModel")(modelUrl.asInstanceOf[js.Any], options.asInstanceOf[js.Any], tfio.asInstanceOf[js.Any])).asInstanceOf[js.Promise[GraphModel[String | IOHandler]]]
  inline def loadGraphModel(modelUrl: IOHandler, options: LoadOptions): js.Promise[GraphModel[String | IOHandler]] = (^.asInstanceOf[js.Dynamic].applyDynamic("loadGraphModel")(modelUrl.asInstanceOf[js.Any], options.asInstanceOf[js.Any])).asInstanceOf[js.Promise[GraphModel[String | IOHandler]]]
  inline def loadGraphModel(modelUrl: IOHandler, options: LoadOptions, tfio: Typeofio): js.Promise[GraphModel[String | IOHandler]] = (^.asInstanceOf[js.Dynamic].applyDynamic("loadGraphModel")(modelUrl.asInstanceOf[js.Any], options.asInstanceOf[js.Any], tfio.asInstanceOf[js.Any])).asInstanceOf[js.Promise[GraphModel[String | IOHandler]]]
  
  inline def loadGraphModelSync(modelSource: js.Tuple2[ModelJSON, /* Weights */ js.typedarray.ArrayBuffer]): GraphModel[IOHandlerSync] = ^.asInstanceOf[js.Dynamic].applyDynamic("loadGraphModelSync")(modelSource.asInstanceOf[js.Any]).asInstanceOf[GraphModel[IOHandlerSync]]
  inline def loadGraphModelSync(modelSource: IOHandlerSync): GraphModel[IOHandlerSync] = ^.asInstanceOf[js.Dynamic].applyDynamic("loadGraphModelSync")(modelSource.asInstanceOf[js.Any]).asInstanceOf[GraphModel[IOHandlerSync]]
  inline def loadGraphModelSync(modelSource: ModelArtifacts): GraphModel[IOHandlerSync] = ^.asInstanceOf[js.Dynamic].applyDynamic("loadGraphModelSync")(modelSource.asInstanceOf[js.Any]).asInstanceOf[GraphModel[IOHandlerSync]]
  
  type Url = String | IOHandler | IOHandlerSync
  
  /** NOTE: Conditional type definitions are impossible to translate to Scala.
    * See https://www.typescriptlang.org/docs/handbook/2/conditional-types.html for an intro.
    * This RHS of the type alias is guess work. You should cast if it's not correct in your case.
    * TS definition: {{{
    T extends string ? @tensorflow/tfjs-core.@tensorflow/tfjs-core/dist/io/types.IOHandler : T
    }}}
    */
  type UrlIOHandler[T /* <: Url */] = T
}
